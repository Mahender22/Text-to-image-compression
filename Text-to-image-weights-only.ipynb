{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7359293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8396befa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                         image_data\n",
      "0     0.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...\n",
      "1     1.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...\n",
      "2    10.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...\n",
      "3   100.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...\n",
      "4  1000.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...\n"
     ]
    }
   ],
   "source": [
    "project_path = r'C:\\Users\\Mahender\\Documents\\MS docs\\Final docs\\Rowan University\\sem4\\ADV topics in DS LLM\\Project\\Text-to-image-for-compression'\n",
    "\n",
    "# # 10 images 0 to 9 - 1 each\n",
    "# img_path = project_path + r'\\\\Data\\Data-10-images'\n",
    "# model_name = 'nn-10-0to9.h5'\n",
    "# grayscale_path = img_path + '-gray'\n",
    "\n",
    "# # 10 images of 1\n",
    "# img_path = r'C:\\Users\\Mahender\\Documents\\MS docs\\Final docs\\Rowan University\\sem4\\ADV topics in DS LLM\\Project\\Text-to-image-for-compression\\Data\\Data-10-images-1'\n",
    "# model_name = 'nn-10-1.h5'\n",
    "# grayscale_path = img_path + '-gray'\n",
    "\n",
    "# All images of 1 - 10772 images\n",
    "img_path = r'C:\\Users\\Mahender\\Documents\\MS docs\\Final docs\\Rowan University\\sem4\\ADV topics in DS LLM\\Project\\Text-to-image-for-compression\\Data\\dataset\\1\\1'\n",
    "model_name = 'nn-all-1.h5'\n",
    "grayscale_path = img_path + '-gray'\n",
    "\n",
    "# # All images 0 to 9 - 118503 images\n",
    "# img_path = r'C:\\Users\\Mahender\\Documents\\MS docs\\Final docs\\Rowan University\\sem4\\ADV topics in DS LLM\\Project\\Text-to-image-for-compression\\Data\\Data-all-images'\n",
    "# model_name = 'nn-all.h5'\n",
    "# grayscale_path = img_path + '-gray'\n",
    "\n",
    "\n",
    "model_save_path = r'C:\\Users\\Mahender\\Documents\\MS docs\\Final docs\\Rowan University\\sem4\\ADV topics in DS LLM\\Project\\Text-to-image-for-compression'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(grayscale_path):\n",
    "    os.makedirs(grayscale_path)\n",
    "\n",
    "image_extension = '.png'\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop through all directories and subdirectories\n",
    "for root, dirs, files in os.walk(img_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(image_extension):\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                \n",
    "                # If the image is in RGBA, convert transparent pixels to white\n",
    "                if img.mode == 'RGBA':\n",
    "                    white_bg = Image.new('RGB', img.size, (255, 255, 255))\n",
    "                    white_bg.paste(img, mask=img.split()[3])\n",
    "                    # Convert the image to grayscale (L mode)\n",
    "                    img = white_bg.convert('L')\n",
    "                else:\n",
    "                    # If the image is already grayscale or RGB, just convert it to grayscale\n",
    "                    img = img.convert('L')\n",
    "                \n",
    "                # Convert image to numpy array and save to DataFrame\n",
    "                img_data = np.array(img)\n",
    "                label = file_path.split('\\\\')[-1]\n",
    "                data.append({\"label\": label, \"image_data\": img_data})\n",
    "\n",
    "                # Save the converted grayscale image\n",
    "                save_path = os.path.join(grayscale_path, file)\n",
    "                img.save(save_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error opening {file_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b2cc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels from the dataframe\n",
    "labels = df['label'].values\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# The onehot_encoded array will be used as input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db2525ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all images are grayscale and of size\n",
    "X = df['image_data'].values\n",
    "\n",
    "# Normalize pixel values\n",
    "X = np.array([x.flatten() / 255.0 for x in X])  # Flatten the images and normalize pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73ac8556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 32)                344768    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 784)               25872     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 371,696\n",
      "Trainable params: 371,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "def create_model(input_dim, output_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(32, activation='relu', input_dim=input_dim))  # First dense layer\n",
    "#     model.add(layers.Dense(512, activation='relu'))  # Hidden dense layer\n",
    "#     model.add(layers.Dense(1024, activation='relu'))  # Hidden dense layer\n",
    "    model.add(layers.Dense(32, activation='relu'))  # Hidden dense layer\n",
    "    model.add(layers.Dense(output_dim, activation='sigmoid'))  # Output layer (flattened image)\n",
    "    return model\n",
    "\n",
    "# Assume one-hot encoded labels have shape (num_samples, num_labels)\n",
    "input_dim = onehot_encoded.shape[1]\n",
    "output_dim = 28 * 28\n",
    "\n",
    "model = create_model(input_dim, output_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e40741b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1200\n",
      "1852/1852 [==============================] - 65s 35ms/step - loss: 0.0552 - accuracy: 0.0000e+00\n",
      "Epoch 2/1200\n",
      "1852/1852 [==============================] - 64s 34ms/step - loss: 0.0465 - accuracy: 0.0000e+00\n",
      "Epoch 3/1200\n",
      "1852/1852 [==============================] - 64s 34ms/step - loss: 0.0458 - accuracy: 0.0000e+00\n",
      "Epoch 4/1200\n",
      "1852/1852 [==============================] - 64s 34ms/step - loss: 0.0443 - accuracy: 8.4386e-06\n",
      "Epoch 5/1200\n",
      "1852/1852 [==============================] - 66s 35ms/step - loss: 0.0421 - accuracy: 0.0000e+00\n",
      "Epoch 6/1200\n",
      "1852/1852 [==============================] - 65s 35ms/step - loss: 0.0388 - accuracy: 0.0000e+00\n",
      "Epoch 7/1200\n",
      "1852/1852 [==============================] - 65s 35ms/step - loss: 0.0362 - accuracy: 0.0000e+00\n",
      "Epoch 8/1200\n",
      "1852/1852 [==============================] - 65s 35ms/step - loss: 0.0333 - accuracy: 0.0000e+00\n",
      "Epoch 9/1200\n",
      "1852/1852 [==============================] - 64s 35ms/step - loss: 0.0307 - accuracy: 0.0000e+00\n",
      "Epoch 10/1200\n",
      "1852/1852 [==============================] - 66s 36ms/step - loss: 0.0289 - accuracy: 0.0000e+00\n",
      "Epoch 11/1200\n",
      "1852/1852 [==============================] - 64s 35ms/step - loss: 0.0272 - accuracy: 0.0000e+00\n",
      "Epoch 12/1200\n",
      "1852/1852 [==============================] - 65s 35ms/step - loss: 0.0258 - accuracy: 2.5316e-05\n",
      "Epoch 13/1200\n",
      "1852/1852 [==============================] - 65s 35ms/step - loss: 0.0246 - accuracy: 4.8100e-04\n",
      "Epoch 14/1200\n",
      "1852/1852 [==============================] - 64s 35ms/step - loss: 0.0235 - accuracy: 0.0029\n",
      "Epoch 15/1200\n",
      "1852/1852 [==============================] - 64s 35ms/step - loss: 0.0226 - accuracy: 0.0102\n",
      "Epoch 16/1200\n",
      "1852/1852 [==============================] - 64s 34ms/step - loss: 0.0217 - accuracy: 0.0208\n",
      "Epoch 17/1200\n",
      "1852/1852 [==============================] - 65s 35ms/step - loss: 0.0210 - accuracy: 0.0373\n",
      "Epoch 18/1200\n",
      "1852/1852 [==============================] - 67s 36ms/step - loss: 0.0203 - accuracy: 0.0572\n",
      "Epoch 19/1200\n",
      "1852/1852 [==============================] - 68s 36ms/step - loss: 0.0198 - accuracy: 0.0814\n",
      "Epoch 20/1200\n",
      "1852/1852 [==============================] - 66s 36ms/step - loss: 0.0193 - accuracy: 0.1064\n",
      "Epoch 21/1200\n",
      "1852/1852 [==============================] - 65s 35ms/step - loss: 0.0188 - accuracy: 0.1358\n",
      "Epoch 22/1200\n",
      "1852/1852 [==============================] - 64s 34ms/step - loss: 0.0185 - accuracy: 0.1672\n",
      "Epoch 23/1200\n",
      "1852/1852 [==============================] - 63s 34ms/step - loss: 0.0181 - accuracy: 0.2028\n",
      "Epoch 24/1200\n",
      "1852/1852 [==============================] - 63s 34ms/step - loss: 0.0177 - accuracy: 0.2394\n",
      "Epoch 25/1200\n",
      "1852/1852 [==============================] - 63s 34ms/step - loss: 0.0174 - accuracy: 0.2760\n",
      "Epoch 26/1200\n",
      "1852/1852 [==============================] - 63s 34ms/step - loss: 0.0172 - accuracy: 0.3108\n",
      "Epoch 27/1200\n",
      "1852/1852 [==============================] - 63s 34ms/step - loss: 0.0169 - accuracy: 0.3470\n",
      "Epoch 28/1200\n",
      "1852/1852 [==============================] - 65s 35ms/step - loss: 0.0167 - accuracy: 0.3801\n",
      "Epoch 29/1200\n",
      "1852/1852 [==============================] - 71s 38ms/step - loss: 0.0164 - accuracy: 0.4144\n",
      "Epoch 30/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0162 - accuracy: 0.4461\n",
      "Epoch 31/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0160 - accuracy: 0.4764\n",
      "Epoch 32/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0159 - accuracy: 0.5087\n",
      "Epoch 33/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0157 - accuracy: 0.5351\n",
      "Epoch 34/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0156 - accuracy: 0.5639\n",
      "Epoch 35/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0154 - accuracy: 0.5870\n",
      "Epoch 36/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0153 - accuracy: 0.6131\n",
      "Epoch 37/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0151 - accuracy: 0.6342\n",
      "Epoch 38/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0150 - accuracy: 0.6562\n",
      "Epoch 39/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0149 - accuracy: 0.6747\n",
      "Epoch 40/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0148 - accuracy: 0.6945\n",
      "Epoch 41/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0147 - accuracy: 0.7114\n",
      "Epoch 42/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0146 - accuracy: 0.7286\n",
      "Epoch 43/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0145 - accuracy: 0.7432\n",
      "Epoch 44/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0144 - accuracy: 0.7596\n",
      "Epoch 45/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0143 - accuracy: 0.7707\n",
      "Epoch 46/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0143 - accuracy: 0.7844\n",
      "Epoch 47/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0142 - accuracy: 0.7959\n",
      "Epoch 48/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0141 - accuracy: 0.8080\n",
      "Epoch 49/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0140 - accuracy: 0.8165\n",
      "Epoch 50/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0140 - accuracy: 0.8266\n",
      "Epoch 51/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0139 - accuracy: 0.8361\n",
      "Epoch 52/1200\n",
      "1852/1852 [==============================] - 73s 39ms/step - loss: 0.0138 - accuracy: 0.8453\n",
      "Epoch 53/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0138 - accuracy: 0.8530\n",
      "Epoch 54/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0137 - accuracy: 0.8593\n",
      "Epoch 55/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0137 - accuracy: 0.8671\n",
      "Epoch 56/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0136 - accuracy: 0.8751\n",
      "Epoch 57/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0135 - accuracy: 0.8797\n",
      "Epoch 58/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0135 - accuracy: 0.8870\n",
      "Epoch 59/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0134 - accuracy: 0.8914\n",
      "Epoch 60/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0134 - accuracy: 0.8974\n",
      "Epoch 61/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0134 - accuracy: 0.9017\n",
      "Epoch 62/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0133 - accuracy: 0.9066\n",
      "Epoch 63/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0133 - accuracy: 0.9111\n",
      "Epoch 64/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0132 - accuracy: 0.9155\n",
      "Epoch 65/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0132 - accuracy: 0.9192\n",
      "Epoch 66/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0131 - accuracy: 0.9232\n",
      "Epoch 67/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0131 - accuracy: 0.9269\n",
      "Epoch 68/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0131 - accuracy: 0.9295\n",
      "Epoch 69/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0130 - accuracy: 0.9322\n",
      "Epoch 70/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0130 - accuracy: 0.9357\n",
      "Epoch 71/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0130 - accuracy: 0.9371\n",
      "Epoch 72/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0129 - accuracy: 0.9399\n",
      "Epoch 73/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0129 - accuracy: 0.9427\n",
      "Epoch 74/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0129 - accuracy: 0.9451\n",
      "Epoch 75/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0128 - accuracy: 0.9468\n",
      "Epoch 76/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0128 - accuracy: 0.9492\n",
      "Epoch 77/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0128 - accuracy: 0.9515\n",
      "Epoch 78/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0127 - accuracy: 0.9537\n",
      "Epoch 79/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0127 - accuracy: 0.9547\n",
      "Epoch 80/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0127 - accuracy: 0.9568\n",
      "Epoch 81/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0126 - accuracy: 0.9585\n",
      "Epoch 82/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0126 - accuracy: 0.9605\n",
      "Epoch 83/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0126 - accuracy: 0.9617\n",
      "Epoch 84/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0126 - accuracy: 0.9636\n",
      "Epoch 85/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0125 - accuracy: 0.9646\n",
      "Epoch 86/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0125 - accuracy: 0.9661\n",
      "Epoch 87/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0125 - accuracy: 0.9669\n",
      "Epoch 88/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0125 - accuracy: 0.9688\n",
      "Epoch 89/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0124 - accuracy: 0.9690\n",
      "Epoch 90/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0124 - accuracy: 0.9706\n",
      "Epoch 91/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0124 - accuracy: 0.9715\n",
      "Epoch 92/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0124 - accuracy: 0.9728\n",
      "Epoch 93/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0123 - accuracy: 0.9733\n",
      "Epoch 94/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0123 - accuracy: 0.9747\n",
      "Epoch 95/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0123 - accuracy: 0.9749\n",
      "Epoch 96/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0123 - accuracy: 0.9765\n",
      "Epoch 97/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0122 - accuracy: 0.9775\n",
      "Epoch 98/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0122 - accuracy: 0.9780\n",
      "Epoch 99/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0122 - accuracy: 0.9783\n",
      "Epoch 100/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0122 - accuracy: 0.9791\n",
      "Epoch 101/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0122 - accuracy: 0.9798\n",
      "Epoch 102/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0121 - accuracy: 0.9803\n",
      "Epoch 103/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0121 - accuracy: 0.9810\n",
      "Epoch 104/1200\n",
      "1852/1852 [==============================] - 71s 38ms/step - loss: 0.0121 - accuracy: 0.9818\n",
      "Epoch 105/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0121 - accuracy: 0.9820\n",
      "Epoch 106/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0121 - accuracy: 0.9826\n",
      "Epoch 107/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0120 - accuracy: 0.9832\n",
      "Epoch 108/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0120 - accuracy: 0.9839\n",
      "Epoch 109/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0120 - accuracy: 0.9839\n",
      "Epoch 110/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0120 - accuracy: 0.9847\n",
      "Epoch 111/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0120 - accuracy: 0.9850\n",
      "Epoch 112/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0120 - accuracy: 0.9856\n",
      "Epoch 113/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0119 - accuracy: 0.9863\n",
      "Epoch 114/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0119 - accuracy: 0.9861\n",
      "Epoch 115/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0119 - accuracy: 0.9871\n",
      "Epoch 116/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0119 - accuracy: 0.9872\n",
      "Epoch 117/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0119 - accuracy: 0.9874\n",
      "Epoch 118/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0119 - accuracy: 0.9877\n",
      "Epoch 119/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0118 - accuracy: 0.9882\n",
      "Epoch 120/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0118 - accuracy: 0.9885\n",
      "Epoch 121/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0118 - accuracy: 0.9891\n",
      "Epoch 122/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0118 - accuracy: 0.9891\n",
      "Epoch 123/1200\n",
      "1852/1852 [==============================] - 72s 39ms/step - loss: 0.0118 - accuracy: 0.9894\n",
      "Epoch 124/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0118 - accuracy: 0.9899\n",
      "Epoch 125/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0118 - accuracy: 0.9901\n",
      "Epoch 126/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0117 - accuracy: 0.9905\n",
      "Epoch 127/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0117 - accuracy: 0.9906\n",
      "Epoch 128/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0117 - accuracy: 0.9909\n",
      "Epoch 129/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0117 - accuracy: 0.9911\n",
      "Epoch 130/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0117 - accuracy: 0.9912\n",
      "Epoch 131/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0117 - accuracy: 0.9916\n",
      "Epoch 132/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0117 - accuracy: 0.9918\n",
      "Epoch 133/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0116 - accuracy: 0.9919\n",
      "Epoch 134/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0116 - accuracy: 0.9921\n",
      "Epoch 135/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0116 - accuracy: 0.9926\n",
      "Epoch 136/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0116 - accuracy: 0.9926\n",
      "Epoch 137/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0116 - accuracy: 0.9927\n",
      "Epoch 138/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0116 - accuracy: 0.9929\n",
      "Epoch 139/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0116 - accuracy: 0.9932\n",
      "Epoch 140/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0115 - accuracy: 0.9933\n",
      "Epoch 141/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0115 - accuracy: 0.9934\n",
      "Epoch 142/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0115 - accuracy: 0.9936\n",
      "Epoch 143/1200\n",
      "1852/1852 [==============================] - 71s 38ms/step - loss: 0.0115 - accuracy: 0.9937\n",
      "Epoch 144/1200\n",
      "1852/1852 [==============================] - 73s 40ms/step - loss: 0.0115 - accuracy: 0.9938\n",
      "Epoch 145/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0115 - accuracy: 0.9938\n",
      "Epoch 146/1200\n",
      "1852/1852 [==============================] - 71s 38ms/step - loss: 0.0115 - accuracy: 0.9944\n",
      "Epoch 147/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0115 - accuracy: 0.9942\n",
      "Epoch 148/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0115 - accuracy: 0.9946\n",
      "Epoch 149/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0114 - accuracy: 0.9946\n",
      "Epoch 150/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0114 - accuracy: 0.9947\n",
      "Epoch 151/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0114 - accuracy: 0.9949\n",
      "Epoch 152/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0114 - accuracy: 0.9948\n",
      "Epoch 153/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0114 - accuracy: 0.9950\n",
      "Epoch 154/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0114 - accuracy: 0.9952\n",
      "Epoch 155/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0114 - accuracy: 0.9952\n",
      "Epoch 156/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0114 - accuracy: 0.9953\n",
      "Epoch 157/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0113 - accuracy: 0.9956\n",
      "Epoch 158/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0113 - accuracy: 0.9957\n",
      "Epoch 159/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0113 - accuracy: 0.9958\n",
      "Epoch 160/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0113 - accuracy: 0.9959\n",
      "Epoch 161/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0113 - accuracy: 0.9958\n",
      "Epoch 162/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0113 - accuracy: 0.9959\n",
      "Epoch 163/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0113 - accuracy: 0.9961\n",
      "Epoch 164/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0113 - accuracy: 0.9961\n",
      "Epoch 165/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0113 - accuracy: 0.9964\n",
      "Epoch 166/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0113 - accuracy: 0.9963\n",
      "Epoch 167/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0112 - accuracy: 0.9964\n",
      "Epoch 168/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0112 - accuracy: 0.9966\n",
      "Epoch 169/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0112 - accuracy: 0.9967\n",
      "Epoch 170/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0112 - accuracy: 0.9965\n",
      "Epoch 171/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0112 - accuracy: 0.9967\n",
      "Epoch 172/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0112 - accuracy: 0.9967\n",
      "Epoch 173/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0112 - accuracy: 0.9969\n",
      "Epoch 174/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0112 - accuracy: 0.9969\n",
      "Epoch 175/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0112 - accuracy: 0.9971\n",
      "Epoch 176/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0112 - accuracy: 0.9972\n",
      "Epoch 177/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0112 - accuracy: 0.9971\n",
      "Epoch 178/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0111 - accuracy: 0.9972\n",
      "Epoch 179/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0111 - accuracy: 0.9972\n",
      "Epoch 180/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0111 - accuracy: 0.9974\n",
      "Epoch 181/1200\n",
      "1852/1852 [==============================] - 70s 38ms/step - loss: 0.0111 - accuracy: 0.9975\n",
      "Epoch 182/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0111 - accuracy: 0.9974\n",
      "Epoch 183/1200\n",
      "1852/1852 [==============================] - 70s 37ms/step - loss: 0.0111 - accuracy: 0.9977\n",
      "Epoch 184/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0111 - accuracy: 0.9976\n",
      "Epoch 185/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0111 - accuracy: 0.9977\n",
      "Epoch 186/1200\n",
      "1852/1852 [==============================] - 68s 37ms/step - loss: 0.0111 - accuracy: 0.9977\n",
      "Epoch 187/1200\n",
      "1852/1852 [==============================] - 68s 37ms/step - loss: 0.0111 - accuracy: 0.9978\n",
      "Epoch 188/1200\n",
      "1852/1852 [==============================] - 67s 36ms/step - loss: 0.0111 - accuracy: 0.9978\n",
      "Epoch 189/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0111 - accuracy: 0.9978\n",
      "Epoch 190/1200\n",
      "1852/1852 [==============================] - 69s 37ms/step - loss: 0.0110 - accuracy: 0.9980\n",
      "Epoch 191/1200\n",
      "1852/1852 [==============================] - 67s 36ms/step - loss: 0.0110 - accuracy: 0.9980\n",
      "Epoch 192/1200\n",
      "1852/1852 [==============================] - 67s 36ms/step - loss: 0.0110 - accuracy: 0.9980\n",
      "Epoch 193/1200\n",
      "1852/1852 [==============================] - 66s 36ms/step - loss: 0.0110 - accuracy: 0.9980\n",
      "Epoch 194/1200\n",
      "1852/1852 [==============================] - 96s 52ms/step - loss: 0.0110 - accuracy: 0.9981\n",
      "Epoch 195/1200\n",
      "1852/1852 [==============================] - 78s 42ms/step - loss: 0.0110 - accuracy: 0.9981\n",
      "Epoch 196/1200\n",
      "1852/1852 [==============================] - 67s 36ms/step - loss: 0.0110 - accuracy: 0.9981\n",
      "Epoch 197/1200\n",
      "1852/1852 [==============================] - 66s 36ms/step - loss: 0.0110 - accuracy: 0.9982\n",
      "Epoch 198/1200\n",
      "1852/1852 [==============================] - 68s 36ms/step - loss: 0.0110 - accuracy: 0.9983\n",
      "Epoch 199/1200\n",
      "  13/1852 [..............................] - ETA: 1:08 - loss: 0.0110 - accuracy: 0.9988"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(onehot_encoded, X, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(onehot_encoded, X, epochs=1200, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4392f470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: dense_3, Data type is float32\n",
      "Layer: dense_3, Data type is float32\n",
      "Layer: dense_4, Data type is float32\n",
      "Layer: dense_4, Data type is float32\n",
      "Layer: dense_5, Data type is float32\n",
      "Layer: dense_5, Data type is float32\n"
     ]
    }
   ],
   "source": [
    "def check_if_float32(model):\n",
    "    for layer in model.layers:\n",
    "        weights = layer.get_weights()\n",
    "        if weights:  # Some layers don't have weights\n",
    "            for weight in weights:\n",
    "                dtype = weight.dtype\n",
    "                if dtype == 'float32':\n",
    "                    print(f\"Layer: {layer.name}, Data type is float32\")\n",
    "                else:\n",
    "                    print(f\"Layer: {layer.name}, Data type is {dtype}, not float32\")\n",
    "\n",
    "# Check if the model weights are float32\n",
    "check_if_float32(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dfd8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_save_path + '\\\\saved_weights\\\\' + model_name) # save weights only\n",
    "\n",
    "# Save the architecture\n",
    "model_json = model.to_json()\n",
    "with open(model_save_path + '\\\\saved_architecture\\\\weights', \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d05ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('\\\\nn-10-1.h5')\n",
    "\n",
    "# Load the architecture from the file\n",
    "from keras.models import model_from_json\n",
    "with open(model_save_path + '\\\\saved_architecture\\\\weights', \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    \n",
    "# Load the weights into the new model\n",
    "model.load_weights(model_save_path + '\\\\saved_weights\\\\' + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97e55c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model (necessary if you're going to use it for training or evaluation)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67a3ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                3792128   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 784)               25872     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,819,056\n",
      "Trainable params: 3,819,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c78c5e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 118503, but received input with shape (None, 1)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n       inputs=tf.Tensor(shape=(None, 1), dtype=int32)\n       training=False\n       mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m encoded_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[label_encoder\u001b[38;5;241m.\u001b[39mtransform([label])[\u001b[38;5;241m0\u001b[39m]]])  \u001b[38;5;66;03m# Shape: (1, 1)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Make a prediction using the integer-encoded label\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m predicted_image \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(encoded_label)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Reshape the predicted image and unnormalize it (convert back to 0-255 range)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m predicted_image \u001b[38;5;241m=\u001b[39m predicted_image\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filei4t43qrn.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Mahender\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 118503, but received input with shape (None, 1)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n       inputs=tf.Tensor(shape=(None, 1), dtype=int32)\n       training=False\n       mask=None\n"
     ]
    }
   ],
   "source": [
    "def pixel_accuracy(original, predicted):\n",
    "    \"\"\"\n",
    "    Calculate pixel accuracy based on the mean absolute error between the original and predicted images.\n",
    "\n",
    "    Parameters:\n",
    "    original (np.array): The original image array (shape: height x width).\n",
    "    predicted (np.array): The predicted image array (shape: height x width).\n",
    "\n",
    "    Returns:\n",
    "    float: The pixel accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    original = np.array(original)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Check if the shapes match\n",
    "    if original.shape != predicted.shape:\n",
    "        raise ValueError(\"Original and predicted images must have the same shape.\")\n",
    "    \n",
    "    # Calculate the mean absolute error\n",
    "    mae = np.mean(np.abs(original - predicted))\n",
    "\n",
    "    # Convert MAE to a percentage accuracy (assuming pixel values are in [0, 255])\n",
    "    max_mae = 255\n",
    "    accuracy = 100 * (1 - (mae / max_mae))  # Normalize accuracy to be between 0 and 100\n",
    "\n",
    "    return accuracy  # Return as percentage\n",
    "\n",
    "# Function to get the original image data from the DataFrame\n",
    "def get_original_image_from_df(label, df):\n",
    "    # Locate the row in the DataFrame that matches the label\n",
    "    row = df[df['label'] == label]\n",
    "    if not row.empty:\n",
    "        return row['image_data'].values[0]  # Get the image data\n",
    "    else:\n",
    "        print(\"Label not found in DataFrame.\")\n",
    "        return None\n",
    "\n",
    "# List to hold pixel accuracy values for all predictions\n",
    "all_accuracies = []\n",
    "\n",
    "# Make a prediction\n",
    "# labels_to_predict = df['label'].values\n",
    "labels_to_predict = random.sample(list(df['label'].values), 10)\n",
    "\n",
    "for label in labels_to_predict:\n",
    "    encoded_label = onehot_encoder.transform([[label_encoder.transform([label])[0]]])\n",
    "    predicted_image = model.predict(encoded_label)\n",
    "\n",
    "    # Reshape the predicted image and unnormalize it (convert back to 0-255 range)\n",
    "    predicted_image = predicted_image.reshape((28, 28))\n",
    "    predicted_image = predicted_image * 255.0  # Unnormalize the predicted image\n",
    "\n",
    "    # Get the original image from the DataFrame\n",
    "    original_image = get_original_image_from_df(label, df)\n",
    "\n",
    "    # Check if original image is not None\n",
    "    if original_image is not None:\n",
    "        # Calculate pixel accuracy\n",
    "        accuracy = pixel_accuracy(original_image, predicted_image)\n",
    "        all_accuracies.append(accuracy)\n",
    "        print(f\"Pixel Accuracy for label '{label}': {accuracy:.2f}%\")\n",
    "\n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot original image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_image, cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')  # Hide axis\n",
    "\n",
    "        # Plot predicted image\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(predicted_image, cmap='gray')\n",
    "        plt.title('Predicted Image')\n",
    "        plt.axis('off')  # Hide axis\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Calculate and print the average pixel accuracy\n",
    "average_accuracy = np.mean(all_accuracies)\n",
    "print(f\"\\nAverage Pixel Accuracy over all above images: {average_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1de3ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3704/3704 [==============================] - 3s 736us/step\n",
      "\n",
      "Average Pixel Accuracy over all images: 97.45%\n"
     ]
    }
   ],
   "source": [
    "# List to hold pixel accuracy values for all predictions\n",
    "all_accuracies = []\n",
    "\n",
    "# Encode all labels in the DataFrame\n",
    "encoded_labels = onehot_encoder.transform([[label_encoder.transform([label])[0]] for label in df['label'].values])\n",
    "\n",
    "# Make predictions for all encoded labels at once\n",
    "predicted_images = model.predict(encoded_labels)\n",
    "\n",
    "# Loop over all predicted images and corresponding original images\n",
    "for i, label in enumerate(df['label'].values):\n",
    "    predicted_image = predicted_images[i].reshape((28, 28))  # Reshape the predicted image\n",
    "    predicted_image = predicted_image * 255.0  # Unnormalize the predicted image\n",
    "\n",
    "    # Get the original image from the DataFrame\n",
    "    original_image = get_original_image_from_df(label, df)\n",
    "\n",
    "    # Check if original image is not None\n",
    "    if original_image is not None:\n",
    "        # Calculate pixel accuracy\n",
    "        accuracy = pixel_accuracy(original_image, predicted_image)\n",
    "        all_accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the average pixel accuracy\n",
    "if all_accuracies:\n",
    "    average_accuracy = np.mean(all_accuracies)\n",
    "    print(f\"\\nAverage Pixel Accuracy over all images: {average_accuracy:.2f}%\")\n",
    "else:\n",
    "    print(\"No valid images found for accuracy calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d455761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
